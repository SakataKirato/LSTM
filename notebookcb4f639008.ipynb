{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gc\nimport os\nimport time\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n\n# --------------------\n# Paths\n# --------------------\nSUBMISSION_DIR = \"/kaggle/working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nBEST_MODEL_PATH = os.path.join(SUBMISSION_DIR, \"best_bilstm_stream.pth\")\n\n# --------------------\n# Device\n# --------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)\n\n# --------------------\n# Hyperparameters\n# --------------------\npatch_size = 150_000\nN_sub_patches = 150\n\n# ★ メモリ対策: データを小分けにするサイズ (500万行ずつ処理)\nCHUNK_SIZE = 50_000_000 \noverlap_rate = 0.2\n\n# ★ 3位チームの裏技: TTF調整用の魔法の値\nBEST_Y = 12.625\n\n# --------------------\n# Helper Function: Feature Extraction\n# --------------------\ndef extract_features(x, N_sub):\n    # x: (patch_size,) -> float32\n    L_sub = len(x) // N_sub\n    x = x[:N_sub * L_sub].reshape(N_sub, L_sub)\n    \n    # 高速化のため float32 で計算\n    mean = x.mean(axis=1)\n    std = x.std(axis=1)\n    min_ = x.min(axis=1)\n    max_ = x.max(axis=1)\n    q25 = np.quantile(x, 0.25, axis=1)\n    q50 = np.quantile(x, 0.50, axis=1)\n    q75 = np.quantile(x, 0.75, axis=1)\n    iqr = q75 - q25\n    \n    return np.stack([mean, std, min_, max_, q25, q50, q75, iqr], axis=1)\n\n# --------------------\n# Main: Streaming Processing (ここが心臓部)\n# --------------------\nrootpath = \"../input/LANL-Earthquake-Prediction/\"\ncsv_path = rootpath + \"train.csv\"\n\n# 結果を格納するリスト\nall_X = []\nall_Y = []\nall_earthquake_ids = []  # ★ 追加: 各パッチがどの地震サイクルか記録\n\n# 正規化用のパラメータ（初期値）\ntrain_mean = 0\ntrain_std = 1\ntarget_min = 0\ntarget_max = 1\n\n# CSVを「イテレータ」として開く（まだデータは読み込まれない）\n# dtypeを指定してメモリ節約\nreader = pd.read_csv(\n    csv_path, \n    chunksize=CHUNK_SIZE,\n    dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float32}\n)\n\nprint(\"Starting Chunk Processing...\")\n\nearthquake_id = 0  # 地震サイクルのカウンター\nprev_y_val = None  # 前のy_valを記録\n\nfor chunk_idx, df_chunk in enumerate(tqdm(reader)):\n    \n    # 波形とターゲットを取得\n    x_chunk = df_chunk[\"acoustic_data\"].values.astype(np.float32)\n    y_chunk = df_chunk[\"time_to_failure\"].values.astype(np.float32)\n    \n    # -------------------------------------------------------\n    # 最初のチャンクを使って、正規化パラメータを決める（近似）\n    # 全データをなめると時間がかかるため、最初の5000万行で推定する\n    # -------------------------------------------------------\n    if chunk_idx == 0:\n        train_mean = x_chunk.mean()\n        train_std = x_chunk.std()\n        target_min = y_chunk.min()\n        target_max = y_chunk.max() # 注: 全体のmaxではないが、実用上大きな問題ではない\n        print(f\"Stats estimated: Mean={train_mean:.4f}, Std={train_std:.4f}\")\n\n    # 正規化 (In-place)\n    x_chunk -= train_mean\n    x_chunk /= train_std\n    \n    # パッチ生成ループ\n    step = int(patch_size * (1 - overlap_rate))\n    \n    chunk_feats = []\n    chunk_targets = []\n    chunk_eq_ids = []  # ★ 追加\n    \n    for i in range(0, len(x_chunk) - patch_size, step):\n        # ターゲット（パッチの最後）\n        y_val = y_chunk[i + patch_size]\n        \n        # ★ 地震サイクルの境界検出（TTFが増加したらリセット）\n        if prev_y_val is not None and y_val > prev_y_val:\n            earthquake_id += 1\n            print(f\"  → Earthquake cycle {earthquake_id} detected\")\n        prev_y_val = y_val\n        \n        # 地震発生直後の不連続点を含むパッチを除外\n        if y_val > y_chunk[i]:\n            continue\n            \n        # 特徴量抽出\n        x_raw = x_chunk[i : i + patch_size]\n        feats = extract_features(x_raw, N_sub_patches)\n        \n        chunk_feats.append(feats)\n        chunk_targets.append(y_val)\n        chunk_eq_ids.append(earthquake_id)  # ★ 追加\n    \n    # 結果をリストに追加\n    if len(chunk_feats) > 0:\n        all_X.append(np.array(chunk_feats))\n        all_Y.append(np.array(chunk_targets))\n        all_earthquake_ids.append(np.array(chunk_eq_ids))  # ★ 追加\n    \n    # メモリ解放！\n    del df_chunk, x_chunk, y_chunk, chunk_feats, chunk_targets, chunk_eq_ids\n    gc.collect()\n\nprint(\"Processing complete. Concatenating...\")\n\n# リストを結合して1つのnumpy配列にする\nX_final = np.concatenate(all_X, axis=0).astype(np.float32)\nY_final = np.concatenate(all_Y, axis=0).astype(np.float32)\nearthquake_ids = np.concatenate(all_earthquake_ids, axis=0)  # ★ 追加\n\nprint(f\"Total earthquake cycles detected: {earthquake_ids.max() + 1}\")\n\n# --------------------\n# ★ 3位チームの裏技: TTF調整\n# --------------------\nprint(f\"\\nApplying 3rd place TTF adjustment (BEST_Y={BEST_Y})...\")\n\nY_adjusted = np.zeros_like(Y_final)\n\nfor eq_id in range(earthquake_ids.max() + 1):\n    # この地震サイクルのマスク\n    mask = (earthquake_ids == eq_id)\n    segment_count = mask.sum()\n    \n    if segment_count == 0:\n        continue\n    \n    # 各セグメントの調整TTFを計算\n    # BEST_Y から線形に減少 (BEST_Y → 0)\n    segment_indices = np.arange(segment_count)\n    adjusted_ttf = BEST_Y - (BEST_Y / segment_count) * segment_indices\n    \n    Y_adjusted[mask] = adjusted_ttf\n    \n    if eq_id < 3:  # 最初の3サイクルだけ表示\n        original_max = Y_final[mask].max()\n        print(f\"  Earthquake {eq_id}: {segment_count} segments, \"\n              f\"Original max TTF: {original_max:.2f}s → Adjusted: {BEST_Y:.3f}s\")\n\nY_final = Y_adjusted\n\n# Yの正規化 (MinMax)\nY_final = (Y_final - Y_final.min()) / (Y_final.max() - Y_final.min())\n\nprint(f\"\\nFinal Data Shape: X={X_final.shape}, Y={Y_final.shape}\")\nprint(f\"Memory Usage of X: {X_final.nbytes / 1024**2:.2f} MB\")\nprint(f\"Adjusted TTF range: [{Y_final.min():.4f}, {Y_final.max():.4f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 1000 # 特徴量が軽いのでエポック回せます\n",
    "learning_rate = 1e-4\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0  # Dropout率\n",
    "\n",
    "# --------------------\n",
    "# Global Seed Control\n",
    "# --------------------\n",
    "SEED = 42  # ★ ここを変更するだけで全体のシードが変わります\n",
    "\n",
    "# --------------------\n",
    "# Reproducibility Settings\n",
    "# --------------------\n",
    "def set_seed(seed):\n",
    "    \"\"\"GPU使用時でも再現性を保証する設定\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # multi-GPU対応\n",
    "    \n",
    "    # CuDNNの決定論的動作を有効化\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Python hash seedも固定（環境変数として設定推奨）\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# --------------------\n",
    "# Dataset & Model (軽量版)\n",
    "# --------------------\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "# DataLoader用のworker初期化関数（再現性のため）\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"各DataLoaderワーカーでシードを固定\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Split\n",
    "indices = np.arange(len(X_final))\n",
    "np.random.seed(SEED)  # ★ SEED変数を使用\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.9 * len(indices))\n",
    "\n",
    "train_ds = SimpleDataset(X_final[indices[:split]], Y_final[indices[:split]])\n",
    "valid_ds = SimpleDataset(X_final[indices[split:]], Y_final[indices[split:]])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    generator=torch.Generator().manual_seed(SEED)  # ★ SEED変数を使用\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# モデル定義: 重み付きプーリング（Attention）\n",
    "# --------------------\n",
    "class BiLSTM(nn.Module):\n",
    "    \"\"\"BiLSTM with Weighted Pooling Attention\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # LSTM層\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout if num_layers > 1 else 0  # 1層の場合はdropout不要\n",
    "        )\n",
    "        \n",
    "        # Attention層（各時刻の重要度を計算）\n",
    "        # 入力: (batch, seq_len, hidden*2) → 出力: (batch, seq_len, 1)\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 最終予測層\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size) = (batch, 150, 8)\n",
    "        \n",
    "        # LSTM処理\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out: (batch, seq_len, hidden*2) = (batch, 150, 256)\n",
    "        \n",
    "        # Attention weights計算\n",
    "        attn_scores = self.attention(lstm_out)\n",
    "        # attn_scores: (batch, seq_len, 1) = (batch, 150, 1)\n",
    "        \n",
    "        # Softmaxで確率分布に変換（各時刻の重要度）\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        # attn_weights: (batch, seq_len, 1) = (batch, 150, 1)\n",
    "        \n",
    "        # 重み付き和（重要な時刻を重視して集約）\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        # context: (batch, hidden*2) = (batch, 256)\n",
    "        \n",
    "        # Dropout + 最終予測\n",
    "        out = self.dropout(context)\n",
    "        out = self.fc(out).squeeze(1)\n",
    "        # out: (batch,)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# モデル初期化\n",
    "model = BiLSTM(8, hidden_size, num_layers, dropout=dropout).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Training Loop\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "print(\"Start Training...\")\n",
    "print(f\"Model: BiLSTM with Weighted Pooling Attention\")\n",
    "print(f\"Parameters: hidden={hidden_size}, layers={num_layers}, dropout={dropout}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_ds)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            val_loss += criterion(pred, y).item() * x.size(0)\n",
    "    val_loss /= len(valid_ds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train: {train_loss:.4f} | Valid: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  → Best model saved! (Val Loss: {best_val:.4f})\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:59:38.381401Z",
     "iopub.status.busy": "2025-12-29T09:59:38.380761Z",
     "iopub.status.idle": "2025-12-29T10:00:31.697146Z",
     "shell.execute_reply": "2025-12-29T10:00:31.696481Z",
     "shell.execute_reply.started": "2025-12-29T09:59:38.381367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": "# --------------------\n# Inference\n# --------------------\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\n\n# モデルの準備\nmodel = BiLSTM(8, hidden_size, num_layers, dropout=dropout).to(device)\nmodel.load_state_dict(torch.load(BEST_MODEL_PATH))\nmodel.eval()\n\n# 提出用ファイルの読み込み\nsample_submission = pd.read_csv(rootpath + \"sample_submission.csv\")\nSUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n\n# テストデータの読み込みと予測\ntest_preds = []\n\nprint(\"Starting Inference...\")\n\n# ★重要: sample_submissionの順序通りにファイルを読み込む\nfor seg_id in tqdm(sample_submission[\"seg_id\"]):\n    # ファイルパスを作成\n    file_path = os.path.join(rootpath, \"test\", seg_id + \".csv\")\n    \n    # 読み込み\n    df = pd.read_csv(file_path, dtype={\"acoustic_data\": np.int16})\n    x = df[\"acoustic_data\"].values.astype(np.float32)\n    \n    # 1. 正規化 (学習時と同じパラメータを使用！)\n    x -= train_mean \n    x /= train_std\n    \n    # 2. 特徴量抽出 (学習時と同じ関数を使用)\n    feats = extract_features(x, N_sub_patches) # shape: (150, 8)\n    \n    # 3. テンソル化 (バッチ次元を追加: [1, 150, 8])\n    x_tensor = torch.tensor(feats).unsqueeze(0).to(device)\n    \n    # 4. 予測\n    with torch.no_grad():\n        pred = model(x_tensor).item()\n    \n    test_preds.append(pred)\n\n# 結果を格納\n# ★ 3位チームの逆正規化: BEST_Yベースで復元\ntest_preds = np.array(test_preds)\n\n# 正規化を解除 (0-1 → 0-BEST_Y)\ntest_preds = test_preds * BEST_Y\n\nprint(f\"\\nPrediction statistics:\")\nprint(f\"  Min: {test_preds.min():.4f}\")\nprint(f\"  Max: {test_preds.max():.4f}\")\nprint(f\"  Mean: {test_preds.mean():.4f}\")\nprint(f\"  (All values should be in range [0, {BEST_Y}])\")\n\nsample_submission[\"time_to_failure\"] = test_preds\nsample_submission.to_csv(SUBMISSION_PATH, index=False)\n\nprint(f\"\\nSubmission saved to: {SUBMISSION_PATH}\")\nprint(sample_submission.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 875412,
     "sourceId": 11000,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}