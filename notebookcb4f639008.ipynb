{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------\n",
    "# Reproducibility Settings\n",
    "# --------------------\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"GPU使用時でも再現性を保証する設定\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # multi-GPU対応\n",
    "    \n",
    "    # CuDNNの決定論的動作を有効化\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Python hash seedも固定（環境変数として設定推奨）\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --------------------\n",
    "# Paths\n",
    "# --------------------\n",
    "SUBMISSION_DIR = \"/kaggle/working\"\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "BEST_MODEL_PATH = os.path.join(SUBMISSION_DIR, \"best_bilstm_stream.pth\")\n",
    "\n",
    "# --------------------\n",
    "# Device\n",
    "# --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# --------------------\n",
    "# Hyperparameters\n",
    "# --------------------\n",
    "patch_size = 150_000\n",
    "N_sub_patches = 150\n",
    "\n",
    "# ★ メモリ対策: データを小分けにするサイズ (500万行ずつ処理)\n",
    "CHUNK_SIZE = 50_000_000 \n",
    "overlap_rate = 0.2\n",
    "\n",
    "# --------------------\n",
    "# Helper Function: Feature Extraction\n",
    "# --------------------\n",
    "def extract_features(x, N_sub):\n",
    "    # x: (patch_size,) -> float32\n",
    "    L_sub = len(x) // N_sub\n",
    "    x = x[:N_sub * L_sub].reshape(N_sub, L_sub)\n",
    "    \n",
    "    # 高速化のため float32 で計算\n",
    "    mean = x.mean(axis=1)\n",
    "    std = x.std(axis=1)\n",
    "    min_ = x.min(axis=1)\n",
    "    max_ = x.max(axis=1)\n",
    "    q25 = np.quantile(x, 0.25, axis=1)\n",
    "    q50 = np.quantile(x, 0.50, axis=1)\n",
    "    q75 = np.quantile(x, 0.75, axis=1)\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    return np.stack([mean, std, min_, max_, q25, q50, q75, iqr], axis=1)\n",
    "\n",
    "# --------------------\n",
    "# Main: Streaming Processing (ここが心臓部)\n",
    "# --------------------\n",
    "rootpath = \"../input/LANL-Earthquake-Prediction/\"\n",
    "csv_path = rootpath + \"train.csv\"\n",
    "\n",
    "# 結果を格納するリスト\n",
    "all_X = []\n",
    "all_Y = []\n",
    "\n",
    "# 正規化用のパラメータ（初期値）\n",
    "train_mean = 0\n",
    "train_std = 1\n",
    "target_min = 0\n",
    "target_max = 1\n",
    "\n",
    "# CSVを「イテレータ」として開く（まだデータは読み込まれない）\n",
    "# dtypeを指定してメモリ節約\n",
    "reader = pd.read_csv(\n",
    "    csv_path, \n",
    "    chunksize=CHUNK_SIZE,\n",
    "    dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float32}\n",
    ")\n",
    "\n",
    "print(\"Starting Chunk Processing...\")\n",
    "\n",
    "for chunk_idx, df_chunk in enumerate(tqdm(reader)):\n",
    "    \n",
    "    # 波形とターゲットを取得\n",
    "    x_chunk = df_chunk[\"acoustic_data\"].values.astype(np.float32)\n",
    "    y_chunk = df_chunk[\"time_to_failure\"].values.astype(np.float32)\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 最初のチャンクを使って、正規化パラメータを決める（近似）\n",
    "    # 全データをなめると時間がかかるため、最初の5000万行で推定する\n",
    "    # -------------------------------------------------------\n",
    "    if chunk_idx == 0:\n",
    "        train_mean = x_chunk.mean()\n",
    "        train_std = x_chunk.std()\n",
    "        target_min = y_chunk.min()\n",
    "        target_max = y_chunk.max() # 注: 全体のmaxではないが、実用上大きな問題ではない\n",
    "        print(f\"Stats estimated: Mean={train_mean:.4f}, Std={train_std:.4f}\")\n",
    "\n",
    "    # 正規化 (In-place)\n",
    "    x_chunk -= train_mean\n",
    "    x_chunk /= train_std\n",
    "    \n",
    "    # パッチ生成ループ\n",
    "    # チャンクごとに処理するため、チャンクの継ぎ目のデータは捨てることになるが\n",
    "    # データ量が膨大なので学習への影響は軽微として無視する\n",
    "    \n",
    "    step = int(patch_size * (1 - overlap_rate))\n",
    "    \n",
    "    chunk_feats = []\n",
    "    chunk_targets = []\n",
    "    \n",
    "    for i in range(0, len(x_chunk) - patch_size, step):\n",
    "        # ターゲット（パッチの最後）\n",
    "        y_val = y_chunk[i + patch_size]\n",
    "        \n",
    "        # 地震発生直後の不連続点を含むパッチを除外\n",
    "        # (パッチの最後の時間が、開始時間より増えていたらリセットが起きている)\n",
    "        if y_val > y_chunk[i]:\n",
    "            continue\n",
    "            \n",
    "        # 特徴量抽出\n",
    "        x_raw = x_chunk[i : i + patch_size]\n",
    "        feats = extract_features(x_raw, N_sub_patches)\n",
    "        \n",
    "        chunk_feats.append(feats)\n",
    "        chunk_targets.append(y_val)\n",
    "    \n",
    "    # 結果をリストに追加\n",
    "    if len(chunk_feats) > 0:\n",
    "        all_X.append(np.array(chunk_feats))\n",
    "        all_Y.append(np.array(chunk_targets))\n",
    "    \n",
    "    # メモリ解放！\n",
    "    del df_chunk, x_chunk, y_chunk, chunk_feats, chunk_targets\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Processing complete. Concatenating...\")\n",
    "\n",
    "# リストを結合して1つのnumpy配列にする\n",
    "# ここでメモリを使うが、特徴量化されているためサイズは元の1/1000以下\n",
    "X_final = np.concatenate(all_X, axis=0).astype(np.float32)\n",
    "Y_final = np.concatenate(all_Y, axis=0).astype(np.float32)\n",
    "\n",
    "# Yの正規化 (MinMax)\n",
    "Y_final = (Y_final - target_min) / (target_max - target_min)\n",
    "\n",
    "print(f\"Final Data Shape: X={X_final.shape}, Y={Y_final.shape}\")\n",
    "print(f\"Memory Usage of X: {X_final.nbytes / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 1000 # 特徴量が軽いのでエポック回せます\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "# --------------------\n",
    "# Dataset & Model (軽量版)\n",
    "# --------------------\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "# DataLoader用のworker初期化関数（再現性のため）\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"各DataLoaderワーカーでシードを固定\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Split\n",
    "indices = np.arange(len(X_final))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.9 * len(indices))\n",
    "\n",
    "train_ds = SimpleDataset(X_final[indices[:split]], Y_final[indices[:split]])\n",
    "valid_ds = SimpleDataset(X_final[indices[split:]], Y_final[indices[split:]])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    generator=torch.Generator().manual_seed(42)  # シャッフル用のジェネレータ\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "\n",
    "# モデル定義\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :]).squeeze(1)\n",
    "        return out\n",
    "\n",
    "model = BiLSTM(8, hidden_size, num_layers).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Training Loop\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_ds)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            val_loss += criterion(pred, y).item() * x.size(0)\n",
    "    val_loss /= len(valid_ds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train: {train_loss:.4f} | Valid: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:59:38.381401Z",
     "iopub.status.busy": "2025-12-29T09:59:38.380761Z",
     "iopub.status.idle": "2025-12-29T10:00:31.697146Z",
     "shell.execute_reply": "2025-12-29T10:00:31.696481Z",
     "shell.execute_reply.started": "2025-12-29T09:59:38.381367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Inference\n",
    "# --------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# モデルの準備\n",
    "model = BiLSTM(8, hidden_size, num_layers).to(device)\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# 提出用ファイルの読み込み\n",
    "sample_submission = pd.read_csv(rootpath + \"sample_submission.csv\")\n",
    "SUBMISSION_PATH = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "# テストデータの読み込みと予測\n",
    "test_preds = []\n",
    "\n",
    "print(\"Starting Inference...\")\n",
    "\n",
    "# ★重要: sample_submissionの順序通りにファイルを読み込む\n",
    "for seg_id in tqdm(sample_submission[\"seg_id\"]):\n",
    "    # ファイルパスを作成\n",
    "    file_path = os.path.join(rootpath, \"test\", seg_id + \".csv\")\n",
    "    \n",
    "    # 読み込み\n",
    "    df = pd.read_csv(file_path, dtype={\"acoustic_data\": np.int16})\n",
    "    x = df[\"acoustic_data\"].values.astype(np.float32)\n",
    "    \n",
    "    # 1. 正規化 (学習時と同じパラメータを使用！)\n",
    "    # 先ほどのコードで計算した train_mean, train_std を使う\n",
    "    # 変数が残っていない場合は、学習ログを見て手動で代入してください (例: train_mean = 4.51...)\n",
    "    x -= train_mean \n",
    "    x /= train_std\n",
    "    \n",
    "    # 2. 特徴量抽出 (学習時と同じ関数を使用)\n",
    "    # extract_features関数は学習コード内で定義したものを使います\n",
    "    feats = extract_features(x, N_sub_patches) # shape: (25, 8)\n",
    "    \n",
    "    # 3. テンソル化 (バッチ次元を追加: [1, 25, 8])\n",
    "    x_tensor = torch.tensor(feats).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 4. 予測\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_tensor).item()\n",
    "    \n",
    "    test_preds.append(pred)\n",
    "\n",
    "# 結果を格納\n",
    "# Yの逆正規化 (学習時と同じ min/max を使用)\n",
    "test_preds = np.array(test_preds)\n",
    "test_preds = test_preds * (target_max - target_min) + target_min\n",
    "\n",
    "sample_submission[\"time_to_failure\"] = test_preds\n",
    "sample_submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {SUBMISSION_PATH}\")\n",
    "print(sample_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 875412,
     "sourceId": 11000,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
